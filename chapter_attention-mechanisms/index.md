# Dikkat Mekanizmaları
:label:`chap_attention`

Bir primatın görme sisteminin optik siniri, beynin tam olarak işleyebildiği şeyi aşan devasa duyusal giriş alır. Neyse ki, tüm uyaranlar eşit yaratılmaz. Odaklanma ve bilinç konsantrasyonu primatların karmaşık görsel ortamda avlar ve yırtıcı hayvanlar gibi ilgi çekici nesnelere dikkati çekmesini sağladı. Bilginin sadece küçük bir kısmına dikkat etme yeteneği evrimsel öneme sahiptir ve bu da insanların yaşamasına ve başarılı olmasına izin verir. 

Bilim adamları 19. yüzyıldan beri bilişsel sinirbilim alanında dikkat çekiyor. Bu bölümde, dikkatin görsel bir sahnede nasıl dağıtıldığını açıklayan popüler bir çerçeveyi inceleyerek başlayacağız. Bu çerçevedeki dikkat işlerinden esinlenerek, bu tür dikkat göstermelerinden faydalanan modeller tasarlayacağız. Özellikle, 1964 yılında Nadaraya-Waston çekirdek regresyonu, *dikkat mekanizmaları* ile makine öğreniminin basit bir gösterimidir. 

Ardından, derin öğrenmede dikkat modellerinin tasarımında yaygın olarak kullanılan dikkat işlevlerini tanıtmaya devam edeceğiz. Özellikle, iki yönlü hizalanabilen ve farklılaşabilen derin öğrenmede çığır açan bir dikkat modeli olan *Bahdanau dikkati* tasarlamak için bu işlevleri nasıl kullanacağımızı göstereceğiz. 

Sonunda, daha yeni ile donatılmış
*çok kafa dikkati*
ve *özdikkat* tasarımları, sadece dikkat mekanizmalarına dayanan *transformatör* mimarisini tanımlayacağız. Transformatörler 2017'deki önerilerinden bu yana dil, vizyon, konuşma ve pekiştirme öğrenme alanlarında modern derin öğrenme uygulamalarında yaygındır.

```toc
:maxdepth: 2

attention-cues
nadaraya-watson
attention-scoring-functions
bahdanau-attention
multihead-attention
self-attention-and-positional-encoding
transformer
```
